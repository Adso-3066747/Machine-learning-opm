<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vectores en Machine Learning</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', 'Segoe UI', sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            overflow: hidden;
            height: 100vh;
            color: #ffffff;
        }

        .presentation-container {
            width: 100%;
            height: 100vh;
            position: relative;
        }

        .slide {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            display: flex;
            align-items: center;
            justify-content: center;
            opacity: 0;
            transform: translateX(50px);
            transition: all 0.7s cubic-bezier(0.25, 0.46, 0.45, 0.94);
            padding: 40px;
        }

        .slide.active {
            opacity: 1;
            transform: translateX(0);
        }

        .slide-content {
            background: rgba(255, 255, 255, 0.05);
            backdrop-filter: blur(20px);
            border-radius: 20px;
            padding: 50px;
            max-width: 1000px;
            width: 100%;
            text-align: center;
            box-shadow: 0 25px 50px rgba(0, 0, 0, 0.3);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .slide-content.left-align {
            text-align: left;
        }

        h1 {
            font-size: 3.2em;
            margin-bottom: 20px;
            background: linear-gradient(45deg, #00d4ff, #ff00ff, #ffaa00);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            font-weight: 700;
        }

        h2 {
            font-size: 2.4em;
            color: #00d4ff;
            margin-bottom: 25px;
            font-weight: 600;
        }

        h3 {
            font-size: 1.6em;
            color: #ffaa00;
            margin-bottom: 15px;
            font-weight: 600;
        }

        p, li {
            font-size: 1.1em;
            line-height: 1.7;
            color: #e1e5e9;
            margin-bottom: 12px;
        }

        .subtitle {
            font-size: 1.3em;
            color: #a0a9b5;
            margin-bottom: 30px;
            font-weight: 300;
        }

        .code-block {
            background: rgba(0, 0, 0, 0.4);
            border-left: 4px solid #00d4ff;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Monaco', 'Consolas', monospace;
            font-size: 0.9em;
            color: #a8dadc;
            margin: 20px 0;
            text-align: left;
            overflow-x: auto;
        }

        .formula-box {
            background: linear-gradient(135deg, rgba(255, 0, 255, 0.1), rgba(0, 212, 255, 0.1));
            border: 1px solid rgba(255, 0, 255, 0.3);
            padding: 25px;
            border-radius: 15px;
            margin: 25px 0;
            font-family: 'KaTeX_Main', 'Times New Roman', serif;
            font-size: 1.2em;
            text-align: center;
        }

        .highlight-box {
            background: rgba(255, 170, 0, 0.15);
            border-left: 4px solid #ffaa00;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }

        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 25px 0;
        }

        .three-column {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 25px 0;
        }

        .feature-card {
            background: rgba(255, 255, 255, 0.08);
            padding: 20px;
            border-radius: 12px;
            border: 1px solid rgba(255, 255, 255, 0.1);
            transition: all 0.3s ease;
        }

        .feature-card:hover {
            background: rgba(255, 255, 255, 0.12);
            transform: translateY(-5px);
        }

        .algorithm-box {
            background: rgba(0, 212, 255, 0.1);
            border: 1px solid rgba(0, 212, 255, 0.3);
            padding: 20px;
            border-radius: 10px;
            margin: 15px 0;
        }

        ul {
            text-align: left;
            padding-left: 20px;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            z-index: 1000;
        }

        .nav-btn {
            padding: 15px 30px;
            background: rgba(0, 212, 255, 0.2);
            border: 2px solid rgba(0, 212, 255, 0.5);
            border-radius: 25px;
            cursor: pointer;
            font-size: 1em;
            font-weight: 600;
            color: #00d4ff;
            transition: all 0.3s ease;
            backdrop-filter: blur(10px);
        }

        .nav-btn:hover {
            background: rgba(0, 212, 255, 0.8);
            color: #1a1a2e;
            transform: translateY(-3px);
            box-shadow: 0 10px 20px rgba(0, 212, 255, 0.3);
        }

        .nav-btn:disabled {
            opacity: 0.3;
            cursor: not-allowed;
            transform: none;
        }

        .slide-counter {
            position: fixed;
            top: 30px;
            right: 30px;
            background: rgba(0, 0, 0, 0.3);
            padding: 12px 20px;
            border-radius: 25px;
            font-weight: 600;
            color: #00d4ff;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(0, 212, 255, 0.3);
        }

        .download-btn {
            position: fixed;
            top: 30px;
            left: 30px;
            padding: 12px 20px;
            background: rgba(255, 170, 0, 0.2);
            color: #ffaa00;
            border: 2px solid rgba(255, 170, 0, 0.5);
            border-radius: 25px;
            cursor: pointer;
            font-weight: 600;
            backdrop-filter: blur(10px);
            transition: all 0.3s ease;
        }

        .download-btn:hover {
            background: rgba(255, 170, 0, 0.8);
            color: #1a1a2e;
            transform: translateY(-2px);
        }

        .icon {
            font-size: 1.5em;
            margin-right: 10px;
        }

        .progress-bar {
            position: fixed;
            bottom: 0;
            left: 0;
            height: 4px;
            background: linear-gradient(90deg, #00d4ff, #ff00ff);
            transition: width 0.3s ease;
            z-index: 1001;
        }

        .math-symbol {
            color: #ff00ff;
            font-weight: bold;
            font-size: 1.1em;
        }

        .vector-notation {
            color: #00d4ff;
            font-weight: bold;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .comparison-table th {
            background: rgba(0, 212, 255, 0.1);
            color: #00d4ff;
            font-weight: 600;
        }

        .center-content {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            height: 100%;
        }
    </style>
</head>
<body>
    <div class="presentation-container">
        <button class="download-btn" onclick="downloadPresentation()">📥 Descargar HTML</button>
        
        <div class="slide-counter">
            <span id="currentSlide">1</span> / <span id="totalSlides">13</span>
        </div>

        <div class="progress-bar" id="progressBar"></div>

        <!-- Slide 1: Título -->
        <div class="slide active">
            <div class="slide-content center-content">
                <h1><span class="icon">🤖</span>VECTORES EN MACHINE LEARNING</h1>
                <p class="subtitle">Fundamentos Matemáticos para la Inteligencia Artificial</p>
                <div class="two-column" style="margin-top: 40px;">
                    <div class="feature-card">
                        <h3>🎯 Representación de Datos</h3>
                        <p>Cada muestra es un vector de características</p>
                    </div>
                    <div class="feature-card">
                        <h3>⚡ Computación Eficiente</h3>
                        <p>Operaciones matriciales optimizadas</p>
                    </div>
                </div>
                <p style="margin-top: 30px; font-size: 1.1em; color: #a0a9b5;">
                    Desarrollo Teórico-Práctico • 13 Diapositivas
                </p>
            </div>
        </div>

        <!-- Slide 2: Importancia de Vectores en ML -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>🚀 ¿Por qué Vectores en ML?</h2>
                <div class="two-column">
                    <div>
                        <h3>Representan Datos</h3>
                        <div class="highlight-box">
                            <p>Cada muestra es un vector de características:</p>
                            <div class="code-block">
Casa → [área, habitaciones, baños, antigüedad]
    → [120, 3, 2, 5]
                            </div>
                        </div>
                        
                        <h3>Modelan Relaciones</h3>
                        <p>Capturan similaridades y distancias entre datos</p>
                    </div>
                    <div>
                        <h3>Facilitan Cálculos</h3>
                        <p>Permiten operaciones matriciales eficientes</p>
                        
                        <h3>Optimización</h3>
                        <div class="algorithm-box">
                            <p><strong>Los algoritmos de ML son procesos de optimización vectorial</strong></p>
                            <p>Navegamos en espacios de alta dimensión para encontrar parámetros óptimos</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 3: Espacios Vectoriales -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>🌌 Espacios Vectoriales en ML</h2>
                <div class="three-column">
                    <div class="feature-card">
                        <h3>🎨 Espacio de Características</h3>
                        <p><strong>Donde viven los datos de entrada</strong></p>
                        <ul>
                            <li>Cada dimensión = una característica</li>
                            <li>Cada punto = una muestra</li>
                            <li>Alta dimensionalidad común</li>
                        </ul>
                    </div>
                    <div class="feature-card">
                        <h3>⚙️ Espacio de Parámetros</h3>
                        <p><strong>Donde viven los pesos del modelo</strong></p>
                        <ul>
                            <li>Optimización = navegar este espacio</li>
                            <li>Gradientes = direcciones de mejora</li>
                            <li>Mínimos locales/globales</li>
                        </ul>
                    </div>
                    <div class="feature-card">
                        <h3>🔮 Espacio de Embedding</h3>
                        <p><strong>Representaciones aprendidas</strong></p>
                        <ul>
                            <li>Word embeddings</li>
                            <li>Características latentes</li>
                            <li>Dimensiones semánticamente significativas</li>
                        </ul>
                    </div>
                </div>
                
                <div class="highlight-box">
                    <h3>🔥 Problema: Maldición de la Dimensionalidad</h3>
                    <p>En espacios de alta dimensión, los puntos tienden a estar equidistantes, haciendo difícil la distinción entre similares y diferentes.</p>
                </div>
            </div>
        </div>

        <!-- Slide 4: Producto Escalar -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>📊 Producto Escalar (Dot Product)</h2>
                <div class="formula-box">
                    <span class="vector-notation">v⃗ · u⃗</span> = <span class="math-symbol">Σᵢ</span> vᵢuᵢ = v₁u₁ + v₂u₂ + ... + vₙuₙ
                </div>
                
                <div class="two-column">
                    <div>
                        <h3>🎯 Aplicaciones en ML</h3>
                        <div class="algorithm-box">
                            <p><strong>1. Predicción Lineal:</strong></p>
                            <p><span class="vector-notation">ŷ = w⃗ · x⃗ + b</span></p>
                        </div>
                        <div class="algorithm-box">
                            <p><strong>2. Similaridad:</strong></p>
                            <p>Mide qué tan "parecidos" son dos vectores</p>
                        </div>
                        <div class="algorithm-box">
                            <p><strong>3. Attention (Transformers):</strong></p>
                            <p><span class="vector-notation">Attention(Q,K,V) = softmax(QKᵀ)V</span></p>
                        </div>
                    </div>
                    <div>
                        <h3>💻 Ejemplo Práctico</h3>
                        <div class="code-block">
import numpy as np

# Datos de una casa
casa = np.array([120, 3, 2, 5])
# [área, habitaciones, baños, antigüedad]

pesos = np.array([1000, 5000, 3000, -500])
# pesos del modelo

# Predicción de precio
precio_predicho = np.dot(casa, pesos)
print(f"Precio: ${precio_predicho}")
# Resultado: $141,500
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 5: Normas Vectoriales -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>📏 Normas Vectoriales</h2>
                <div class="two-column">
                    <div>
                        <h3>🔢 Norma L2 (Euclidiana)</h3>
                        <div class="formula-box">
                            <span class="vector-notation">||v⃗||₂</span> = √(v₁² + v₂² + ... + vₙ²)
                        </div>
                        <p><strong>Uso:</strong> Distancias, regularización L2, normalización</p>
                    </div>
                    <div>
                        <h3>🏙️ Norma L1 (Manhattan)</h3>
                        <div class="formula-box">
                            <span class="vector-notation">||v⃗||₁</span> = |v₁| + |v₂| + ... + |vₙ|
                        </div>
                        <p><strong>Uso:</strong> Regularización L1, sparsity, robustez</p>
                    </div>
                </div>
                
                <h3>🎯 Aplicaciones Principales</h3>
                <div class="three-column">
                    <div class="feature-card">
                        <h4>🛡️ Regularización</h4>
                        <p>Penalizar pesos grandes para evitar overfitting</p>
                        <div class="code-block" style="font-size: 0.8em;">
L2: λ||w||₂²
L1: λ||w||₁
                        </div>
                    </div>
                    <div class="feature-card">
                        <h4>📐 Normalización</h4>
                        <p>Escalar vectores a magnitud unitaria</p>
                        <div class="code-block" style="font-size: 0.8em;">
v̂ = v⃗/||v⃗||
                        </div>
                    </div>
                    <div class="feature-card">
                        <h4>📊 Distancias</h4>
                        <p>Medir similaridad entre puntos</p>
                        <div class="code-block" style="font-size: 0.8em;">
d(v⃗,u⃗) = ||v⃗-u⃗||
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 6: Distancias y Similaridad -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>🎯 Distancias y Similaridad</h2>
                <div class="two-column">
                    <div>
                        <h3>📐 Distancia Euclidiana</h3>
                        <div class="formula-box">
                            d(v⃗, u⃗) = <span class="vector-notation">||v⃗ - u⃗||₂</span>
                        </div>
                        <p>Distancia "directa" en el espacio</p>
                        
                        <h3>🧭 Similaridad Coseno</h3>
                        <div class="formula-box">
                            cos_sim(v⃗, u⃗) = <span class="math-symbol">(v⃗ · u⃗) / (||v⃗|| ||u⃗||)</span>
                        </div>
                        <p>Mide el ángulo entre vectores</p>
                    </div>
                    <div>
                        <h3>🚀 Aplicaciones</h3>
                        <div class="algorithm-box">
                            <h4>K-Nearest Neighbors</h4>
                            <p>Encontrar los k vecinos más cercanos</p>
                        </div>
                        <div class="algorithm-box">
                            <h4>Clustering</h4>
                            <p>Agrupar puntos similares (K-means, etc.)</p>
                        </div>
                        <div class="algorithm-box">
                            <h4>Sistemas de Recomendación</h4>
                            <p>Similaridad entre usuarios/items</p>
                        </div>
                    </div>
                </div>
                
                <div class="highlight-box">
                    <p><strong>💡 Insight:</strong> La similaridad coseno es independiente de la magnitud, solo considera la dirección. Ideal para textos y vectores de diferentes escalas.</p>
                </div>
            </div>
        </div>

        <!-- Slide 7: Regresión Lineal Vectorial -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>📈 Regresión Lineal Vectorial</h2>
                <div class="two-column">
                    <div>
                        <h3>📋 Formulación</h3>
                        <div class="formula-box">
                            <p><strong>Hipótesis:</strong> h(x⃗) = <span class="vector-notation">w⃗ᵀx⃗</span> + b</p>
                            <p><strong>Costo:</strong> J(w⃗) = <span class="math-symbol">½m</span> <span class="vector-notation">||Xw⃗ - y⃗||²</span></p>
                            <p><strong>Gradiente:</strong> ∇J(w⃗) = <span class="math-symbol">1/m</span> <span class="vector-notation">Xᵀ(Xw⃗ - y⃗)</span></p>
                        </div>
                        
                        <div class="algorithm-box">
                            <h4>🔄 Gradient Descent</h4>
                            <p><span class="vector-notation">w⃗ₜ₊₁ = w⃗ₜ - α∇J(w⃗ₜ)</span></p>
                        </div>
                    </div>
                    <div>
                        <h3>💻 Implementación</h3>
                        <div class="code-block" style="font-size: 0.75em;">
class LinearRegression:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.weights = None
        self.bias = 0
    
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        
        for i in range(1000):
            # Predicción vectorial
            y_pred = np.dot(X, self.weights) + self.bias
            
            # Gradientes vectoriales
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # Actualización
            self.weights -= self.lr * dw
            self.bias -= self.lr * db
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 8: Word Embeddings -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>🔤 Word Embeddings</h2>
                <div class="two-column">
                    <div>
                        <h3>🎨 Concepto</h3>
                        <p>Representar palabras como vectores densos donde <strong>palabras similares tienen vectores cercanos</strong></p>
                        
                        <div class="algorithm-box">
                            <h4>Word2Vec</h4>
                            <p>Aprende embeddings basados en contexto</p>
                        </div>
                        
                        <div class="highlight-box">
                            <h4>🔮 Operaciones Semánticas</h4>
                            <div class="formula-box">
                                <span class="vector-notation">king - man + woman ≈ queen</span><br>
                                <span class="vector-notation">Paris - France + Italy ≈ Rome</span>
                            </div>
                        </div>
                    </div>
                    <div>
                        <h3>💻 Ejemplo Conceptual</h3>
                        <div class="code-block" style="font-size: 0.8em;">
# Vectores de ejemplo (dimensión 5)
king = np.array([0.2, 0.8, 0.3, -0.1, 0.9])
queen = np.array([0.1, 0.7, 0.4, -0.2, 0.8])
man = np.array([0.5, 0.1, 0.2, 0.8, 0.3])
woman = np.array([0.3, 0.2, 0.1, 0.7, 0.4])

# Aritmética vectorial semántica
result = king - man + woman

# Verificar similaridad
cosine_sim = np.dot(result, queen) / (
    np.linalg.norm(result) * np.linalg.norm(queen)
)
print(f"Similaridad con queen: {cosine_sim:.3f}")
                        </div>
                        
                        <h3>🎯 Aplicaciones</h3>
                        <ul>
                            <li>Procesamiento de lenguaje natural</li>
                            <li>Sistemas de recomendación</li>
                            <li>Análisis de sentimientos</li>
                            <li>Traducción automática</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 9: Optimización Vectorial -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>⚡ Optimización Vectorial</h2>
                <div class="three-column">
                    <div class="feature-card">
                        <h3>🎯 Gradient Descent</h3>
                        <div class="formula-box" style="font-size: 1em;">
                            <span class="vector-notation">w⃗ₜ₊₁ = w⃗ₜ - α∇J(w⃗ₜ)</span>
                        </div>
                        <p><strong>Variantes:</strong></p>
                        <ul style="font-size: 0.9em;">
                            <li><strong>Batch GD:</strong> Todo el dataset</li>
                            <li><strong>SGD:</strong> Una muestra aleatoria</li>
                            <li><strong>Mini-batch:</strong> Subconjunto pequeño</li>
                        </ul>
                    </div>
                    
                    <div class="feature-card">
                        <h3>🚀 Momentum</h3>
                        <p>Acelera usando "velocidad":</p>
                        <div class="formula-box" style="font-size: 0.9em;">
                            <span class="vector-notation">v⃗ₜ₊₁ = βv⃗ₜ + α∇J(w⃗ₜ)</span><br>
                            <span class="vector-notation">w⃗ₜ₊₁ = w⃗ₜ - v⃗ₜ₊₁</span>
                        </div>
                        <p style="font-size: 0.9em;">Ayuda a superar mínimos locales</p>
                    </div>
                    
                    <div class="feature-card">
                        <h3>🎛️ Adam</h3>
                        <p>Combina momentum + adaptación:</p>
                        <div class="formula-box" style="font-size: 0.8em;">
                            <span class="vector-notation">m⃗ₜ = β₁m⃗ₜ₋₁ + (1-β₁)∇J</span><br>
                            <span class="vector-notation">v⃗ₜ = β₂v⃗ₜ₋₁ + (1-β₂)(∇J)²</span><br>
                            <span class="vector-notation">w⃗ₜ₊₁ = w⃗ₜ - α(m⃗ₜ/√(v⃗ₜ + ε))</span>
                        </div>
                        <p style="font-size: 0.9em;">Optimizador más popular en deep learning</p>
                    </div>
                </div>
                
                <div class="highlight-box">
                    <h3>🔑 Principio Clave</h3>
                    <p>Todos estos algoritmos navegan en el espacio de parámetros usando información vectorial (gradientes) para encontrar configuraciones que minimizen la función de pérdida.</p>
                </div>
            </div>
        </div>

        <!-- Slide 10: Transformaciones -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>🔄 Transformaciones Vectoriales</h2>
                <div class="two-column">
                    <div>
                        <h3>📊 Normalización</h3>
                        <div class="algorithm-box">
                            <h4>Min-Max Scaling</h4>
                            <div class="formula-box" style="font-size: 0.9em;">
                                x'ᵢ = (xᵢ - min(x⃗))/(max(x⃗) - min(x⃗))
                            </div>
                        </div>
                        
                        <div class="algorithm-box">
                            <h4>Z-Score Normalization</h4>
                            <div class="formula-box" style="font-size: 0.9em;">
                                x'ᵢ = (xᵢ - μ)/σ
                            </div>
                        </div>
                    </div>
                    <div>
                        <h3>📉 Reducción de Dimensionalidad</h3>
                        <div class="algorithm-box">
                            <h4>PCA (Principal Component Analysis)</h4>
                            <p style="font-size: 0.9em;">Encuentra direcciones de máxima varianza:</p>
                            <ol style="font-size: 0.8em;">
                                <li>Centrar: X̃ = X - μ</li>
                                <li>Covarianza: C = (1/n)X̃ᵀX̃</li>
                                <li>Eigendecomposición: C = PΛPᵀ</li>
                                <li>Proyección: Y = XP</li>
                            </ol>
                        </div>
                        
                        <div class="code-block" style="font-size: 0.7em;">
def normalize_features(X):
    # Z-score normalization
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    return (X - mean) / std

X_norm = normalize_features(X_train)
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 11: Casos Prácticos - Recomendación -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>🎬 Caso Práctico: Sistema de Recomendación</h2>
                <div class="two-column">
                    <div>
                        <h3>🎯 Concepto</h3>
                        <p>Usar <strong>similaridad coseno</strong> entre vectores de usuario para encontrar personas con gustos similares</p>
                        
                        <div class="highlight-box">
                            <h4>👥 Collaborative Filtering</h4>
                            <p>"Los usuarios similares a ti también disfrutaron..."</p>
                        </div>
                        
                        <div class="algorithm-box">
                            <h4>🔢 Matriz Usuario-Item</h4>
                            <table class="comparison-table" style="font-size: 0.8em;">
                                <tr><th></th><th>Film A</th><th>Film B</th><th>Film C</th></tr>
                                <tr><td>User 1</td><td>5</td><td>4</td><td>0</td></tr>
                                <tr><td>User 2</td><td>0</td><td>3</td><td>4</td></tr>
                                <tr><td>User 3</td><td>4</td><td>0</td><td>5</td></tr>
                            </table>
                        </div>
                    </div>
                    <div>
                        <h3>💻 Implementación</h3>
                        <div class="code-block" style="font-size: 0.65em;">
class RecommenderSystem:
    def __init__(self):
        self.user_vectors = None
    
    def fit(self, user_item_matrix):
        self.user_vectors = user_item_matrix
    
    def recommend(self, user_id, n_rec=5):
        user_vector = self.user_vectors[user_id]
        
        # Calcular similaridades coseno
        similarities = cosine_similarity(
            [user_vector], self.user_vectors
        )[0]
        
        # Usuarios similares (top 5)
        similar_users = np.argsort(similarities)[-6:-1]
        
        # Recomendar items populares
        recommendations = []
        for similar_user in similar_users:
            user_items = self.user_vectors[similar_user]
            recs = np.where(user_items > 0)[0]
            recommendations.extend(recs)
        
        return list(set(recommendations))[:n_rec]

# Uso
recommender = RecommenderSystem()
recommender.fit(ratings_matrix)
recs = recommender.recommend(user_id=0)
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 12: Deep Learning y Vectores -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>🧠 Deep Learning y Vectores</h2>
                <div class="three-column">
                    <div class="feature-card">
                        <h3>🔍 CNNs</h3>
                        <p><strong>Filtros:</strong> Vectores que se convolucionan con la entrada</p>
                        <p><strong>Feature maps:</strong> Representaciones vectoriales de características detectadas</p>
                        <div class="code-block" style="font-size: 0.7em;">
# Convolución = producto escalar
output[i,j] = Σ filter * patch[i,j]
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <h3>🔄 RNNs</h3>
                        <p><strong>Estados ocultos:</strong> Vectores que mantienen memoria temporal</p>
                        <p><strong>Embeddings:</strong> Representaciones densas de tokens</p>
                        <div class="code-block" style="font-size: 0.7em;">
h_t = tanh(W_h h_{t-1} + W_x x_t + b)
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <h3>🎯 Transformers</h3>
                        <p><strong>Query, Key, Value:</strong> Vectores que determinan atención</p>
                        <p><strong>Multi-head attention:</strong> Múltiples espacios de atención</p>
                        <div class="code-block" style="font-size: 0.7em;">
Attention = softmax(QK^T/√d_k)V
                        </div>
                    </div>
                </div>
                
                <div class="highlight-box">
                    <h3>🔑 Attention Mechanism</h3>
                    <div class="code-block" style="font-size: 0.8em;">
def scaled_dot_product_attention(Q, K, V):
    """
    Q: Query vectors [batch, seq_len, d_k]
    K: Key vectors [batch, seq_len, d_k]  
    V: Value vectors [batch, seq_len, d_v]
    """
    d_k = Q.shape[-1]
    
    # Compute attention scores (productos escalares)
    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)
    
    # Apply softmax
    attention_weights = softmax(scores, axis=-1)
    
    # Apply attention to values
    output = np.matmul(attention_weights, V)
    
    return output, attention_weights
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 13: Resumen y Conclusiones -->
        <div class="slide">
            <div class="slide-content left-align">
                <h2>🎯 Conclusiones Clave</h2>
                
                <div class="two-column">
                    <div>
                        <h3>🚀 Principios Fundamentales</h3>
                        <div class="feature-card">
                            <ul>
                                <li><strong>Todo es un vector:</strong> Datos, parámetros, gradientes</li>
                                <li><strong>Operaciones matriciales:</strong> Base de eficiencia computacional</li>
                                <li><strong>Espacios de alta dimensión:</strong> Donde viven los datos reales</li>
                                <li><strong>Similaridades y distancias:</strong> Core de muchos algoritmos</li>
                                <li><strong>Optimización:</strong> Navegación en espacios de parámetros</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div>
                        <h3>⚡ Mejores Prácticas</h3>
                        <div class="algorithm-box">
                            <ul>
                                <li>✅ <strong>Normalizar características</strong> para estabilidad numérica</li>
                                <li>✅ <strong>Usar operaciones vectorizadas</strong> para eficiencia</li>
                                <li>✅ <strong>Entender la geometría</strong> de los datos</li>
                                <li>✅ <strong>Aplicar regularización</strong> para evitar overfitting</li>
                                <li>✅ <strong>Visualizar en menor dimensión</strong> cuando sea posible</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="highlight-box">
                    <h3>🎨 El Arte del Machine Learning</h3>
                    <p style="font-size: 1.2em; text-align: center;">
                        <strong>Los vectores no son solo herramientas matemáticas en ML; son el lenguaje fundamental que permite a las máquinas aprender patrones complejos de los datos.</strong>
                    </p>
                </div>
                
                <div class="formula-box" style="margin-top: 30px;">
                    <h3 style="color: #00d4ff;">🔮 Desde datos hasta inteligencia:</h3>
                    <p style="font-size: 1.1em;">
                        <span class="vector-notation">Datos</span> → <span class="math-symbol">Vectores</span> → <span class="vector-notation">Operaciones</span> → <span class="math-symbol">Optimización</span> → <span class="vector-notation">Inteligencia Artificial</span>
                    </p>
                </div>
                
                <div style="text-align: center; margin-top: 40px;">
                    <h2 style="color: #ffaa00;">¡Gracias por su atención! 🚀</h2>
                    <p style="color: #a0a9b5;">¿Preguntas sobre vectores en ML?</p>
                </div>
            </div>
        </div>

        <div class="navigation">
            <button class="nav-btn" id="prevBtn" onclick="changeSlide(-1)">⬅ Anterior</button>
            <button class="nav-btn" id="nextBtn" onclick="changeSlide(1)">Siguiente ➡</button>
        </div>
    </div>

    <script>
        let currentSlideIndex = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        
        document.getElementById('totalSlides').textContent = totalSlides;
        
        function updateProgressBar() {
            const progress = ((currentSlideIndex + 1) / totalSlides) * 100;
            document.getElementById('progressBar').style.width = progress + '%';
        }
        
        function showSlide(index) {
            slides.forEach(slide => slide.classList.remove('active'));
            slides[index].classList.add('active');
            
            document.getElementById('currentSlide').textContent = index + 1;
            updateProgressBar();
            
            // Update navigation buttons
            document.getElementById('prevBtn').disabled = index === 0;
            document.getElementById('nextBtn').disabled = index === totalSlides - 1;
        }
        
        function changeSlide(direction) {
            const newIndex = currentSlideIndex + direction;
            if (newIndex >= 0 && newIndex < totalSlides) {
                currentSlideIndex = newIndex;
                showSlide(currentSlideIndex);
            }
        }
        
        // Keyboard navigation
        document.addEventListener('keydown', function(e) {
            if (e.key === 'ArrowRight' || e.key === ' ') {
                e.preventDefault();
                changeSlide(1);
            } else if (e.key === 'ArrowLeft') {
                e.preventDefault();
                changeSlide(-1);
            } else if (e.key === 'Home') {
                e.preventDefault();
                currentSlideIndex = 0;
                showSlide(0);
            } else if (e.key === 'End') {
                e.preventDefault();
                currentSlideIndex = totalSlides - 1;
                showSlide(totalSlides - 1);
            }
        });
        
        // Touch/swipe support for mobile
        let touchStartX = 0;
        let touchEndX = 0;
        
        document.addEventListener('touchstart', function(e) {
            touchStartX = e.changedTouches[0].screenX;
        });
        
        document.addEventListener('touchend', function(e) {
            touchEndX = e.changedTouches[0].screenX;
            handleSwipe();
        });
        
        function handleSwipe() {
            const swipeThreshold = 50;
            const diff = touchStartX - touchEndX;
            
            if (Math.abs(diff) > swipeThreshold) {
                if (diff > 0) {
                    // Swipe left - next slide
                    changeSlide(1);
                } else {
                    // Swipe right - previous slide
                    changeSlide(-1);
                }
            }
        }
        
        function downloadPresentation() {
            const htmlContent = document.documentElement.outerHTML;
            const blob = new Blob([htmlContent], { type: 'text/html' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'Vectores-en-Machine-Learning.html';
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
        }
        
        // Initialize
        showSlide(0);
        
        // Preload next slide for smoother transitions
        function preloadSlide(index) {
            if (index >= 0 && index < totalSlides) {
                slides[index].style.transform = 'translateX(0)';
            }
        }
        
        // Auto-save progress in sessionStorage (not localStorage due to restrictions)
        function saveProgress() {
            try {
                sessionStorage.setItem('ml-vectors-slide', currentSlideIndex.toString());
            } catch(e) {
                // Silently fail if sessionStorage not available
            }
        }
        
        function loadProgress() {
            try {
                const saved = sessionStorage.getItem('ml-vectors-slide');
                if (saved !== null) {
                    const slideIndex = parseInt(saved);
                    if (slideIndex >= 0 && slideIndex < totalSlides) {
                        currentSlideIndex = slideIndex;
                        showSlide(currentSlideIndex);
                    }
                }
            } catch(e) {
                // Silently fail if sessionStorage not available
            }
        }
        
        // Load progress on page load
        window.addEventListener('load', loadProgress);
        
        // Save progress when changing slides
        const originalChangeSlide = changeSlide;
        changeSlide = function(direction) {
            originalChangeSlide(direction);
            saveProgress();
        };
    </script>
</body>
</html>